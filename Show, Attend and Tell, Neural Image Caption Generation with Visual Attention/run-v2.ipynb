{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import os\n",
    "\n",
    "# the compute_blue implementation is taken from:\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/transformer/compute_bleu.py\n",
    "from helpers.evaluation import compute_bleu\n",
    "from helpers.files import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "dataset_config = config['dataset']\n",
    "annotation_folder = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('./datasets') + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('./datasets'),\n",
    "                                          origin = dataset_config['annotation_url'],\n",
    "                                          extract = True)\n",
    "    os.remove(annotation_zip)\n",
    "    annotation_file = os.path.dirname(annotation_folder) + '/annotations/captions_train2014.json'\n",
    "else:\n",
    "    annotation_file = './datasets/annotations/captions_train2014.json'\n",
    "\n",
    "# Download image files\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('./datasets') + image_folder):\n",
    "    image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('./datasets'),\n",
    "                                      origin = dataset_config['images_url'],\n",
    "                                      extract = True)\n",
    "    PATH = os.path.dirname(image_zip) + image_folder\n",
    "    os.remove(image_zip)\n",
    "else:\n",
    "    PATH = os.path.abspath('./datasets') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "def load_dataset(annotations, shuffle=True, \n",
    "                 test_size=0.2, num_examples=None):\n",
    "\n",
    "    # variables to store the images url and their corresponding captions.\n",
    "    all_captions = []\n",
    "    all_img_id_vector = []\n",
    "\n",
    "    # for each annotation add sos and eos tokens and map \n",
    "    for annotation in annotations['annotations']:\n",
    "        caption = '<sos> ' + annotation['caption'] + ' <eos>'\n",
    "        image_id = annotation['image_id']\n",
    "        all_img_id_vector.append(image_id)\n",
    "        all_captions.append(caption)\n",
    "    \n",
    "    train_imgs, test_imgs, train_captions, test_captions = train_test_split(all_img_id_vector[:num_examples],\n",
    "                                                                            all_captions[:num_examples], \n",
    "                                                                            test_size=test_size, shuffle=shuffle)\n",
    "    \n",
    "    return train_imgs, test_imgs, train_captions, test_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = dataset_config['num_examples']\n",
    "X_train, X_test, y_train, y_test = load_dataset(annotations, num_examples=num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=256)\n",
    "def get_all_captions_by_image_id(image_id):\n",
    "    captions = []\n",
    "    for caption in annotations['annotations']:\n",
    "        if caption['image_id'] == image_id:\n",
    "            captions.append(caption['caption'])\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man hovering over a chocolate cake on a wooden counter.',\n",
       " 'A man blowing out candles on  a birthday cake',\n",
       " 'an image of a man blowing out the candles on a cake',\n",
       " 'a person blowing out the candles of a cake on a table',\n",
       " 'Man blowing out the candles of cake with beer in background.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = get_all_captions_by_image_id(X_train[0])\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Preprocessing\n",
    "\n",
    "We are going to use inceptionV3 model and hence its processing\n",
    "\n",
    "1. We will load the image from the ID.\n",
    "2. Resize the image to 299x299\n",
    "3. Preprocess using InceptionV3 preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_id):\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Preprocessing\n",
    "\n",
    "As usual, we need to:\n",
    "\n",
    "1. Split, clean and tokenize the texts to build the vocabulary.\n",
    "2. Constrain number of words to most common words and add the rest of words to Out-of-Vocabulary bucket.\n",
    "3. Convert the texts to sequences.\n",
    "4. Pad the texts to maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = dataset_config['top_common_words']\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<UNK>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~\\'')\n",
    "tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "y_train = tokenizer.texts_to_sequences(y_train)\n",
    "y_test = tokenizer.texts_to_sequences(y_test)\n",
    "\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, padding=\"post\")\n",
    "max_length = y_train.shape[-1]\n",
    "\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test, padding=\"post\", maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_id, cap):\n",
    "    img_tensor, _ = preprocess_image(img_id)\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = dataset_config['batch_size']\n",
    "buffer_size = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\"enc_input\": X_train, \"dec_input\"; y_train}, ))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          preprocess, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "epochs = model_config['epochs']\n",
    "embedding_dim = model_config['embedding_dim']\n",
    "units = model_config['units']\n",
    "attention = model_config['attention']\n",
    "\n",
    "train_size = len(X_train)\n",
    "steps_per_epoch = train_size // batch_size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNNEncoder, RNNDecoder\n",
    "\n",
    "model = ImageCaption(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation function consists of the following:\n",
    "\n",
    "- Similar to step function, except we don't do teacher forcing.\n",
    "- Stop predicting when the model predicts the end token.\n",
    "- Store the attention weights for every time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))  #\n",
    "    \n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    image, _ = preprocess_image(image)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    features = encoder(image)\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']], 0)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, hidden, features)\n",
    "        predictions = predictions[:, -1:, :]  # we only care about the last prediction.\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        last_prediction_id = predicted_id.numpy()[0][0]\n",
    "        \n",
    "        if tokenizer.index_word[predicted_id] == '<eos>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        result.append(tokenizer.index_word[last_prediction_id])\n",
    "        \n",
    "        dec_input = tf.concat([dec_input, predicted_id], axis=-1)\n",
    "        \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image_id, result, attention_plot):\n",
    "    image = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2+1, len_result//2 +1, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption(image_id, tokenizer, verbose=True):    \n",
    "    real_captions = get_all_captions_by_image_id(image_id)\n",
    "    result, attention_plot = evaluate(image_id)\n",
    "    pred_caption = ' '.join(result)        \n",
    "    \n",
    "    prediction = [tokenizer.texts_to_sequences([pred_caption])[0]]\n",
    "    references = [tokenizer.texts_to_sequences(real_captions)]\n",
    "    bleu = compute_bleu(references, prediction, max_order=3)\n",
    "    \n",
    "    if verbose:\n",
    "        print ('Real Caption:', real_captions)\n",
    "        print ('Prediction Caption:', pred_caption)\n",
    "        print(\"bleu result:\", bleu)\n",
    "        plot_attention(image_id, result, attention_plot)\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "def caption_by_index(image_index, tokenizer, verbose=True):\n",
    "    image_id = X_test[image_index]\n",
    "    if verbose:\n",
    "        print(\"processing image id:\", image_id, \"that have index of:\", image_index)\n",
    "    caption(image_id, tokenizer, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_by_index(np.random.randint(0, 200), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_img_name_vector)\n",
    "\n",
    "bleu1, bleu2, bleu3, bleu = [], [], [], []\n",
    "for img in test_dataset:\n",
    "    all_bleu = caption(img.numpy(), tokenizer, verbose=False)\n",
    "    bleu.append(all_bleu[0])\n",
    "    bleu1.append(all_bleu[1][0])\n",
    "    bleu2.append(all_bleu[1][1])\n",
    "    bleu3.append(all_bleu[1][2])\n",
    "\n",
    "print(\"bleu: {}\\tbleu1: {}\\tbleu2: {}\\tbleu 3: {}\".format(np.mean(bleu), np.mean(bleu1), np.mean(bleu2), np.mean(bleu3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes to improve the model:\n",
    "\n",
    "- Train on multiple captions of the same image.\n",
    "- Add validation set and exit trying earlier.\n",
    "- Transfer learning of a better embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
