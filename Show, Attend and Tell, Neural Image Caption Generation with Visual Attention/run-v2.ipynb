{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import os\n",
    "\n",
    "# the compute_blue implementation is taken from:\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/transformer/compute_bleu.py\n",
    "from helpers.evaluation import compute_bleu\n",
    "from helpers.files import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "dataset_config = config['dataset']\n",
    "annotation_folder = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('./datasets') + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('./datasets'),\n",
    "                                          origin = dataset_config['annotation_url'],\n",
    "                                          extract = True)\n",
    "    os.remove(annotation_zip)\n",
    "    annotation_file = os.path.dirname(annotation_folder) + '/annotations/captions_train2014.json'\n",
    "else:\n",
    "    annotation_file = './datasets/annotations/captions_train2014.json'\n",
    "\n",
    "# Download image files\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('./datasets') + image_folder):\n",
    "    image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('./datasets'),\n",
    "                                      origin = dataset_config['images_url'],\n",
    "                                      extract = True)\n",
    "    PATH = os.path.dirname(image_zip) + image_folder\n",
    "    os.remove(image_zip)\n",
    "else:\n",
    "    PATH = os.path.abspath('./datasets') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "def load_dataset(annotations, shuffle=True, \n",
    "                 test_size=0.2, num_examples=None):\n",
    "\n",
    "    # variables to store the images url and their corresponding captions.\n",
    "    all_captions = []\n",
    "    all_img_id_vector = []\n",
    "\n",
    "    # for each annotation add sos and eos tokens and map \n",
    "    for annotation in annotations['annotations']:\n",
    "        caption = '<sos> ' + annotation['caption'] + ' <eos>'\n",
    "        image_id = annotation['image_id']\n",
    "        all_img_id_vector.append(image_id)\n",
    "        all_captions.append(caption)\n",
    "    \n",
    "    train_imgs, test_imgs, train_captions, test_captions = train_test_split(all_img_id_vector[:num_examples],\n",
    "                                                                            all_captions[:num_examples], \n",
    "                                                                            test_size=test_size, shuffle=shuffle)\n",
    "    \n",
    "    return train_imgs, test_imgs, train_captions, test_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = dataset_config['num_examples']\n",
    "X_train, X_test, y_train, y_test = load_dataset(annotations, num_examples=num_examples)\n",
    "# create training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=256)\n",
    "def get_all_captions_by_image_id(image_id, tokenizer=None):\n",
    "    captions = []\n",
    "    for caption in annotations['annotations']:\n",
    "        if caption['image_id'] == image_id:\n",
    "            captions.append(caption['caption'])\n",
    "    if tokenizer is None:\n",
    "        return captions\n",
    "    else:\n",
    "        return tokenizer.texts_to_sequences(captions)\n",
    "\n",
    "def get_all_captions_by_image_ids(image_ids, tokenizer=None):\n",
    "    return [get_all_captions_by_image_id(img_id, tokenizer) for img_id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A note is placed near a toilet with it's seat up. \",\n",
       " 'Graffiti on an open door to a restroom stall.',\n",
       " 'A white toilet in a very small room.',\n",
       " 'Black and white photograph of a doorway view into a bathroom.',\n",
       " 'A bathroom door is open and has writing on it.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = get_all_captions_by_image_id(X_train[0])\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Preprocessing\n",
    "\n",
    "We are going to use inceptionV3 model and hence its processing\n",
    "\n",
    "1. We will load the image from the ID.\n",
    "2. Resize the image to 299x299\n",
    "3. Preprocess using InceptionV3 preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_id):\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "def preprocess_images(image_ids):\n",
    "    processed_images = np.zeros((len(image_ids), 299, 299, 3), dtype=np.float32)\n",
    "    for i, img_id in enumerate(image_ids):\n",
    "        processed_images[i], _ = preprocess_image(img_id)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Preprocessing\n",
    "\n",
    "As usual, we need to:\n",
    "\n",
    "1. Split, clean and tokenize the texts to build the vocabulary.\n",
    "2. Constrain number of words to most common words and add the rest of words to Out-of-Vocabulary bucket.\n",
    "3. Convert the texts to sequences.\n",
    "4. Pad the texts to maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = dataset_config['top_common_words']\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<UNK>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~\\'')\n",
    "tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "y_train = tokenizer.texts_to_sequences(y_train)\n",
    "y_valid = tokenizer.texts_to_sequences(y_valid)\n",
    "y_test = tokenizer.texts_to_sequences(y_test)\n",
    "\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, padding=\"post\")\n",
    "max_length = y_train.shape[-1]\n",
    "\n",
    "y_valid = tf.keras.preprocessing.sequence.pad_sequences(y_valid, padding=\"post\", maxlen=max_length)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test, padding=\"post\", maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(enc_input, dec_input,):\n",
    "    enc_input, _ = preprocess_image(enc_input)\n",
    "    return enc_input, dec_input\n",
    "\n",
    "def set_shapes(img, cap, img_shape, cap_shape):\n",
    "    img.set_shape(img_shape)\n",
    "    cap.set_shape(cap_shape)\n",
    "    return img, cap\n",
    "\n",
    "def create_shifted_target(y):\n",
    "    \"remove the start token and append a padding to the end.\"\n",
    "    return y[:, :-1], y[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = dataset_config['batch_size']\n",
    "buffer_size = 1000\n",
    "img_shape = (299, 299, 3)\n",
    "cap_shape = (max_length-1,)\n",
    "\n",
    "dec_input, train_target = create_shifted_target(y_train)\n",
    "attention_weights = np.zeros((len(dec_input)), dtype=np.float32)\n",
    "\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((X_train, dec_input))\n",
    "dataset_target = tf.data.Dataset.from_tensor_slices((train_target, attention_weights))\n",
    "\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_input = dataset_input.map(lambda item1, item2: tf.py_function(\n",
    "                preprocess, [item1, item2], (tf.float32, tf.int32)),\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# when the preprocessing is done in py_function or numpy_function, the shapes are lost!\n",
    "# so we have to set the shape again for the input.\n",
    "dataset_input = dataset_input.map(lambda item1, item2: set_shapes(item1, item2, img_shape, cap_shape))\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_target))\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True).repeat().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input_valid, train_target_valid = create_shifted_target(y_valid)\n",
    "attention_weights = np.zeros((len(dec_input_valid)), dtype=np.float32)\n",
    "\n",
    "dataset_valid_input = tf.data.Dataset.from_tensor_slices((X_valid, dec_input_valid))\n",
    "dataset_valid_target = tf.data.Dataset.from_tensor_slices((train_target_valid, attention_weights))\n",
    "\n",
    "dataset_valid_input = dataset_valid_input.map(lambda item1, item2: tf.py_function(preprocess, [item1, item2], (tf.float32, tf.int32)), \n",
    "                                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid_input = dataset_valid_input.map(lambda item1, item2: set_shapes(item1, item2, img_shape, cap_shape))\n",
    "dataset_valid = tf.data.Dataset.zip((dataset_valid_input, dataset_valid_target)).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "epochs = model_config['epochs']\n",
    "embedding_dim = model_config['embedding_dim']\n",
    "units = model_config['units']\n",
    "attention = model_config['attention']\n",
    "\n",
    "train_size = len(X_train)\n",
    "valid_size = len(X_valid)\n",
    "steps_per_epoch = train_size // batch_size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (None, 8, 8, 2048)\n",
    "# we will reshape it to (None, 64, 2048) and feed it to model.\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64  # we could leave this as -1 as well.\n",
    "inception_shape = (attention_features_shape, features_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ImageCaption\n",
    "\n",
    "# loss to not compute the attention to the actual loss\n",
    "def neglected_loss(y_true, y_pred):\n",
    "    return tf.constant(0, dtype=tf.float32)\n",
    "\n",
    "model = ImageCaption(inception_shape, vocab_size, embedding_dim, units)\n",
    "model.compile(optimizer='adam', loss=[\"sparse_categorical_crossentropy\", neglected_loss], metrics=[['accuracy'], []], loss_weights=[1., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4141 steps, validate for 1035 steps\n",
      "Epoch 1/50\n",
      "4141/4141 [==============================] - 2473s 597ms/step - loss: 0.7224 - output_1_loss: 0.7224 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8656 - val_loss: 0.6198 - val_output_1_loss: 0.6198 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8761\n",
      "Epoch 2/50\n",
      "4141/4141 [==============================] - 1861s 449ms/step - loss: 0.5847 - output_1_loss: 0.5847 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8793 - val_loss: 0.5981 - val_output_1_loss: 0.5981 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8785\n",
      "Epoch 3/50\n",
      "4141/4141 [==============================] - 1860s 449ms/step - loss: 0.5448 - output_1_loss: 0.5448 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8832 - val_loss: 0.5968 - val_output_1_loss: 0.5968 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8795\n",
      "Epoch 4/50\n",
      "4141/4141 [==============================] - 1850s 447ms/step - loss: 0.5145 - output_1_loss: 0.5145 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8867 - val_loss: 0.6043 - val_output_1_loss: 0.6043 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8788\n",
      "Epoch 5/50\n",
      "4141/4141 [==============================] - 1858s 449ms/step - loss: 0.4910 - output_1_loss: 0.4910 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8899 - val_loss: 0.6171 - val_output_1_loss: 0.6171 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8786\n",
      "Epoch 6/50\n",
      "4141/4141 [==============================] - 1861s 449ms/step - loss: 0.4662 - output_1_loss: 0.4662 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8935 - val_loss: 0.6321 - val_output_1_loss: 0.6321 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8779\n",
      "Epoch 7/50\n",
      "4141/4141 [==============================] - 1884s 455ms/step - loss: 0.4457 - output_1_loss: 0.4457 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8968 - val_loss: 0.6632 - val_output_1_loss: 0.6632 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8760\n",
      "Epoch 8/50\n",
      "4141/4141 [==============================] - 1863s 450ms/step - loss: 0.4290 - output_1_loss: 0.4290 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.8996 - val_loss: 0.6697 - val_output_1_loss: 0.6697 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8761\n",
      "Epoch 9/50\n",
      "4141/4141 [==============================] - 1832s 442ms/step - loss: 0.3701 - output_1_loss: 0.3701 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.9103 - val_loss: 0.6519 - val_output_1_loss: 0.6519 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8796\n",
      "Epoch 10/50\n",
      "4141/4141 [==============================] - 1809s 437ms/step - loss: 0.3426 - output_1_loss: 0.3426 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.9158 - val_loss: 0.6615 - val_output_1_loss: 0.6615 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8792\n",
      "Epoch 11/50\n",
      "4140/4141 [============================>.] - ETA: 0s - loss: 0.3293 - output_1_loss: 0.3293 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.9187Restoring model weights from the end of the best epoch.\n",
      "4141/4141 [==============================] - 1810s 437ms/step - loss: 0.3293 - output_1_loss: 0.3293 - output_2_loss: 0.0000e+00 - output_1_accuracy: 0.9187 - val_loss: 0.6728 - val_output_1_loss: 0.6728 - val_output_2_loss: 0.0000e+00 - val_output_1_accuracy: 0.8785\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_size // batch_size\n",
    "validation_steps = valid_size // batch_size\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"./checkpoints/best_model.h5\", save_best_only=True),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=5),\n",
    "             tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, verbose=True)]\n",
    "\n",
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "                    #validation_data=dataset_valid, validation_steps=validation_steps,\n",
    "                    #callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helpers.logging import plot_learning_curve\n",
    "\n",
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation function consists of the following:\n",
    "\n",
    "- Similar to step function, except we don't do teacher forcing.\n",
    "- Store the attention weights for every time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_ids, target_tokenizer, sos_token=\"<sos>\"):\n",
    "    enc_inputs = preprocess_images(img_ids)\n",
    "    y_preds = tf.fill(dims=(len(img_ids), 1), value=target_tokenizer.word_index[sos_token])\n",
    "    attention_weights = np.zeros((len(img_ids), max_length, attention_features_shape))\n",
    "    for i in range(max_length):\n",
    "        pad_size = max_length - y_preds.shape[1]\n",
    "        dec_input = tf.pad(y_preds, [[0, 0], [0, pad_size]])\n",
    "        y_probs_next, step_attention_weights = model.predict((enc_inputs, dec_input))\n",
    "        attention_weights[:, i, :] = step_attention_weights[:, i, :]\n",
    "        y_probs_next = y_probs_next[:, i:i+1]  # we only care about the current state\n",
    "        y_pred_next = tf.argmax(y_probs_next, axis=-1, output_type=tf.int32)\n",
    "        y_preds = tf.concat([y_preds, y_pred_next], axis=1)\n",
    "    return y_preds[:, 1:], attention_weights  # remove the <sos> token from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _ = predict(X_test[:10], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(img_ids, target_tokenizer, sos_token=\"<sos>\"):\n",
    "    real_captions = get_all_captions_by_image_ids(img_ids, tokenizer=target_tokenizer)\n",
    "    predicted_captions, _ = predict(img_ids, target_tokenizer, sos_token=sos_token)\n",
    "    \n",
    "    padding_indices = np.argwhere(results == tokenizer.texts_to_sequences([\"<eos>\"]))[:, 1]\n",
    "    \n",
    "    bleu = compute_bleu(real_captions, predicted_captions.numpy(), padding_indices, max_order=3)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate(X_test[:10], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.4032416324380917\tbleu1: 0.7333333333333333\tbleu2: 0.4\tbleu 3: 0.2235294117647059\n"
     ]
    }
   ],
   "source": [
    "print(\"bleu: {}\\tbleu1: {}\\tbleu2: {}\\tbleu 3: {}\".format(bleu[0], bleu[1][0], bleu[1][1], bleu[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image_id, result, attention_plot):\n",
    "    result = result.split(\" \")\n",
    "    image = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(60, 60))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2+1, len_result//2 +1, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption(image_id, tokenizer, verbose=True):    \n",
    "    real_captions = get_all_captions_by_image_ids([image_id], tokenizer=tokenizer)\n",
    "    predict_caption, attention_weights = predict([image_id], tokenizer)\n",
    "    padding_indices = np.argwhere(predict_caption == tokenizer.texts_to_sequences([\"<eos>\"]))[:, 1]\n",
    "    \n",
    "    bleu = compute_bleu(real_captions, predict_caption.numpy(), padding_indices, max_order=3)\n",
    "    \n",
    "    padding_index = padding_indices[0]\n",
    "    references = tokenizer.sequences_to_texts(real_captions[0])\n",
    "    translation = tokenizer.sequences_to_texts(predict_caption.numpy()[:, :padding_index+1])[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print ('Real Caption:', references)\n",
    "        print ('Prediction Caption:', translation)\n",
    "        print(\"bleu result:\", bleu)\n",
    "        plot_attention(image_id, translation, \n",
    "                       attention_weights[0, :padding_index+1])\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "def caption_by_index(image_index, tokenizer, verbose=True):\n",
    "    image_id = X_test[image_index]\n",
    "    if verbose:\n",
    "        print(\"processing image id:\", image_id, \"that have index of:\", image_index)\n",
    "    caption(image_id, tokenizer, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_by_index(np.random.randint(0, 200), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDos:\n",
    "\n",
    "- Train on multiple captions of the same image.\n",
    "- Transfer learning of a better embedding layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
