{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esbel/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from helpers import load_config\n",
    "from helpers.logging import print_status_bar\n",
    "from helpers.evaluation import compute_bleu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = config[\"dataset\"]\n",
    "path_to_zip = tf.keras.utils.get_file(os.path.join(os.getcwd(), \"datasets\", \"fra-eng.zip\"), origin=dataset_params[\"url\"], extract=True)\n",
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), \"fra.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # make a space between each punctionation\n",
    "    sentence = sentence.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    \n",
    "    sentence = sentence.strip()  # remove spaces\n",
    "    return sentence\n",
    "\n",
    "def preprocess_a_sentence(sentence):\n",
    "    # clean it\n",
    "    sentence = clean_sentence(sentence)\n",
    "    # add the start and end of sequences\n",
    "    return '<sos> {} <eos>'.format(sentence)\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "    # list containing a set of (input, output)\n",
    "    sentence_pairs = [[preprocess_a_sentence(sen) for sen in line.split('\\t')]  for line in lines[:num_examples]]\n",
    "    return zip(*sentence_pairs)\n",
    "\n",
    "def create_shifted_target(y):\n",
    "    \"Remove the start token and append a padding to the end.\"\n",
    "    return y[:, :-1], y[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(lang, top_k=None):\n",
    "    \n",
    "    # we are keeping the punctionation\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, filters='’,?!\"#$%&()*+-/:;=.@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(lang)\n",
    "    # pad the tensors\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\")\n",
    "    return sequences, tokenizer\n",
    "\n",
    "def create_dataset(X, y, batch_size=None, buffer=False, prefetch=tf.data.experimental.AUTOTUNE):\n",
    "    X_set = tf.data.Dataset.from_tensor_slices(X)\n",
    "    y_set = tf.data.Dataset.from_tensor_slices(y[0],)\n",
    "    a_set = tf.data.Dataset.zip((X_set, y_set))\n",
    "    if buffer:\n",
    "        a_set = a_set.shuffle(X[0].shape[0])\n",
    "    if batch_size is not None:\n",
    "        a_set = a_set.batch(batch_size, drop_remainder=True)\n",
    "    return a_set.repeat().prefetch(prefetch)\n",
    "\n",
    "def padded_transform(X, tokenizer, X_max):\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences(X, padding=\"post\", maxlen=X_max)\n",
    "    return X\n",
    "\n",
    "def dataset_padded_transform(X, y, X_tokenizer, y_tokenizer, X_max, y_max):\n",
    "    X = padded_transform(X, X_tokenizer, X_max)\n",
    "    y = padded_transform(y, y_tokenizer, y_max)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(input_lang, target_lang, batch_size, prefetch=tf.data.experimental.AUTOTUNE, \n",
    "            valid_size=0.15, top_k=None):    \n",
    "    \n",
    "    encoder_train, encoder_valid, target_train, target_valid = train_test_split(input_lang, \n",
    "                                                                                target_lang, \n",
    "                                                                                test_size=valid_size)\n",
    "    # build tokenizer\n",
    "    encoder_train, input_tokenizer = get_tokenizer(encoder_train, top_k=top_k)\n",
    "    target_train, target_tokenizer = get_tokenizer(target_train, top_k=top_k)\n",
    "    \n",
    "    # transform and pad\n",
    "    encoder_valid, target_valid = dataset_padded_transform(encoder_valid, target_valid,\n",
    "                                                           input_tokenizer, target_tokenizer,\n",
    "                                                           encoder_train.shape[1], target_train.shape[1])\n",
    "    \n",
    "    decoder_train, y_train = create_shifted_target(target_train)\n",
    "    train_attention_weights = np.zeros((len(decoder_train)), dtype=np.float32)\n",
    "    \n",
    "    decoder_valid, y_valid = create_shifted_target(target_valid)\n",
    "    valid_attention_weights = np.zeros((len(decoder_valid)), dtype=np.float32)\n",
    "    \n",
    "    # create dataset    \n",
    "    train_set = create_dataset((encoder_train, decoder_train),\n",
    "                               (y_train, train_attention_weights),\n",
    "                               batch_size=batch_size, buffer=True,\n",
    "                               prefetch=prefetch)\n",
    "    \n",
    "    valid_set = create_dataset((encoder_valid, decoder_valid),\n",
    "                               (y_valid, valid_attention_weights),\n",
    "                               batch_size=batch_size, prefetch=prefetch)\n",
    "    \n",
    "    # information about the training set:\n",
    "    info = dict(\n",
    "        train_size=encoder_train.shape[0],\n",
    "        train_input_max_pad=encoder_train.shape[1],\n",
    "        train_target_max_pad=target_train.shape[1],\n",
    "        valid_size=encoder_valid.shape[0],\n",
    "    )\n",
    "    return train_set, valid_set, info, input_tokenizer, target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_size': 113648, 'train_input_max_pad': 66, 'train_target_max_pad': 52, 'valid_size': 20056}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "batch_size = dataset_params[\"batch_size\"]\n",
    "num_examples = dataset_params[\"num_examples\"]\n",
    "top_k = dataset_params['top_common_words']\n",
    "\n",
    "# load dataset and split training, validation and testing sets.\n",
    "target_lang, input_lang = load_dataset(path_to_file, num_examples=num_examples)\n",
    "encoder_train, encoder_test, target_train, target_test = train_test_split(input_lang, \n",
    "                                                                          target_lang, \n",
    "                                                                          test_size=0.2)\n",
    "# create training and validation set\n",
    "train_set, valid_set, info, input_tokenizer, target_tokenizer = dataset(encoder_train, target_train, batch_size, \n",
    "                                                                        top_k=top_k)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos> ils se saluèrent mutuellement en soulevant leurs chapeaux <eos>']\n",
      "['<sos> they saluted each other by raising their hats <eos>']\n",
      "['they saluted each other by raising their hats <eos>']\n"
     ]
    }
   ],
   "source": [
    "for x, y, in train_set.take(1):\n",
    "    i = 10\n",
    "    enc_x, dec_x = x\n",
    "    # y, att = y\n",
    "    print(input_tokenizer.sequences_to_texts([enc_x[i].numpy()]))\n",
    "    print(target_tokenizer.sequences_to_texts([dec_x[i].numpy()]))\n",
    "    print(target_tokenizer.sequences_to_texts([y[i].numpy()]))\n",
    "    # print(att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "N = model_config['N']\n",
    "model_depth = model_config['model_depth']\n",
    "num_heads = model_config['num_heads']\n",
    "dff = model_config['dff']\n",
    "dropout_rate = model_config['dropout_rate']\n",
    "epochs = model_config['epochs']\n",
    "\n",
    "\n",
    "steps_per_epoch = info['train_size'] // batch_size\n",
    "validation_steps = info['valid_size'] // batch_size\n",
    "max_input_vocab = len(input_tokenizer.index_word) + 1\n",
    "max_target_vocab = len(target_tokenizer.index_word) + 1\n",
    "input_max_positional_encoding = max_input_vocab\n",
    "target_max_positional_encoding = max_target_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerV2, CustomSchedule\n",
    "transformer = TransformerV2(N, model_depth, num_heads, dff, \n",
    "                            max_input_vocab, max_target_vocab, \n",
    "                            input_max_positional_encoding, target_max_positional_encoding, \n",
    "                            rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "\n",
    "Where are going to use adam optimizer with a custom LR:\n",
    "\n",
    "$$l_{\\text{rate}} = d_{\\text{model}}^{-0.5} * \\text{min}(\\text{step_num}^{-0.5}, \\text{step_num} * \\text{warmup_steps}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(CustomSchedule(model_depth), beta_1=0.9, beta_2=0.98, \n",
    "                                epsilon=1e-9)\n",
    "transformer.compile(optimizer=adam, metrics=['accuracy'], loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1775 steps, validate for 313 steps\n",
      "Epoch 1/25\n",
      "1775/1775 [==============================] - 694s 391ms/step - loss: 1.1313 - accuracy: 0.8831 - val_loss: 0.4765 - val_accuracy: 0.9184\n",
      "Epoch 2/25\n",
      "1775/1775 [==============================] - 669s 377ms/step - loss: 0.4358 - accuracy: 0.9228 - val_loss: 0.4061 - val_accuracy: 0.9250\n",
      "Epoch 3/25\n",
      "1775/1775 [==============================] - 665s 375ms/step - loss: 0.3919 - accuracy: 0.9266 - val_loss: 0.3664 - val_accuracy: 0.9304\n",
      "Epoch 4/25\n",
      "1775/1775 [==============================] - 666s 375ms/step - loss: 0.3526 - accuracy: 0.9320 - val_loss: 0.3453 - val_accuracy: 0.9334\n",
      "Epoch 5/25\n",
      "1775/1775 [==============================] - 667s 376ms/step - loss: 0.3265 - accuracy: 0.9361 - val_loss: 0.3264 - val_accuracy: 0.9369\n",
      "Epoch 6/25\n",
      "1775/1775 [==============================] - 669s 377ms/step - loss: 0.3088 - accuracy: 0.9392 - val_loss: 0.3216 - val_accuracy: 0.9378\n",
      "Epoch 7/25\n",
      "1775/1775 [==============================] - 670s 378ms/step - loss: 0.2961 - accuracy: 0.9416 - val_loss: 0.3163 - val_accuracy: 0.9390\n",
      "Epoch 8/25\n",
      "1775/1775 [==============================] - 669s 377ms/step - loss: 0.2851 - accuracy: 0.9435 - val_loss: 0.3137 - val_accuracy: 0.9395\n",
      "Epoch 9/25\n",
      "1775/1775 [==============================] - 670s 378ms/step - loss: 0.2745 - accuracy: 0.9455 - val_loss: 0.3118 - val_accuracy: 0.9405\n",
      "Epoch 10/25\n",
      "1775/1775 [==============================] - 670s 378ms/step - loss: 0.2656 - accuracy: 0.9472 - val_loss: 0.3140 - val_accuracy: 0.9399\n",
      "Epoch 11/25\n",
      "1775/1775 [==============================] - 665s 375ms/step - loss: 0.2575 - accuracy: 0.9486 - val_loss: 0.3102 - val_accuracy: 0.9405\n",
      "Epoch 12/25\n",
      "1775/1775 [==============================] - 662s 373ms/step - loss: 0.2526 - accuracy: 0.9495 - val_loss: 0.3113 - val_accuracy: 0.9404\n",
      "Epoch 13/25\n",
      "1775/1775 [==============================] - 662s 373ms/step - loss: 0.2437 - accuracy: 0.9512 - val_loss: 0.3123 - val_accuracy: 0.9406\n",
      "Epoch 14/25\n",
      "1775/1775 [==============================] - 662s 373ms/step - loss: 0.2377 - accuracy: 0.9524 - val_loss: 0.3124 - val_accuracy: 0.9409\n",
      "Epoch 15/25\n",
      "1775/1775 [==============================] - 662s 373ms/step - loss: 0.2315 - accuracy: 0.9535 - val_loss: 0.3160 - val_accuracy: 0.9405\n",
      "Epoch 16/25\n",
      "1775/1775 [==============================] - 661s 372ms/step - loss: 0.2264 - accuracy: 0.9544 - val_loss: 0.3168 - val_accuracy: 0.9402\n",
      "Epoch 17/25\n",
      "1775/1775 [==============================] - 659s 371ms/step - loss: 0.2209 - accuracy: 0.9556 - val_loss: 0.3167 - val_accuracy: 0.9401\n",
      "Epoch 18/25\n",
      "1775/1775 [==============================] - 659s 372ms/step - loss: 0.2170 - accuracy: 0.9563 - val_loss: 0.3181 - val_accuracy: 0.9403\n",
      "Epoch 19/25\n",
      "1775/1775 [==============================] - 661s 372ms/step - loss: 0.2125 - accuracy: 0.9570 - val_loss: 0.3123 - val_accuracy: 0.9414\n",
      "Epoch 20/25\n",
      "1775/1775 [==============================] - 664s 374ms/step - loss: 0.2082 - accuracy: 0.9579 - val_loss: 0.3127 - val_accuracy: 0.9413\n",
      "Epoch 21/25\n",
      "1775/1775 [==============================] - 666s 375ms/step - loss: 0.2044 - accuracy: 0.9586 - val_loss: 0.3115 - val_accuracy: 0.9416\n",
      "Epoch 22/25\n",
      "1775/1775 [==============================] - 658s 371ms/step - loss: 0.1998 - accuracy: 0.9595 - val_loss: 0.3161 - val_accuracy: 0.9409\n",
      "Epoch 23/25\n",
      "1775/1775 [==============================] - 653s 368ms/step - loss: 0.1962 - accuracy: 0.9603 - val_loss: 0.3139 - val_accuracy: 0.9412\n",
      "Epoch 24/25\n",
      "1775/1775 [==============================] - 656s 369ms/step - loss: 0.1933 - accuracy: 0.9608 - val_loss: 0.3142 - val_accuracy: 0.9415\n",
      "Epoch 25/25\n",
      "1775/1775 [==============================] - 658s 371ms/step - loss: 0.1899 - accuracy: 0.9614 - val_loss: 0.3117 - val_accuracy: 0.9418\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_set, steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "                          validation_data=valid_set, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The following steps are used for evaluation:\n",
    "\n",
    "- Encode the input sentence using the input tokenizer and add the necessary starting and ending tokens\n",
    "- Create a decoder input and set the first value to start token\n",
    "- The decoder outputs the predictions by looking at the encoder's output and its own output.\n",
    "- Select the last work and calculate the argmax of that.\n",
    "- Concatentane the predicted word of input and refeed the decoder until the end token is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(enc_inputs, target_tokenizer, sos_token=\"<sos>\"):\n",
    "    y_preds = tf.fill(dims=(len(enc_inputs), 1), value=target_tokenizer.word_index[sos_token])\n",
    "    for i in range(info['train_target_max_pad']):\n",
    "        pad_size = info['train_target_max_pad'] - y_preds.shape[1]\n",
    "        dec_input = tf.pad(y_preds, [[0, 0], [0, pad_size]])\n",
    "        y_probs_next = transformer.predict((enc_inputs, dec_input))\n",
    "        y_probs_next = y_probs_next[:, i:i+1]  # we only care about the current state\n",
    "        y_pred_next = tf.argmax(y_probs_next, axis=-1, output_type=tf.int32)\n",
    "        y_preds = tf.concat([y_preds, y_pred_next], axis=1)\n",
    "    return y_preds[:, 1:]  # remove the <sos> token from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentences, translations, input_tokenizer, target_tokenizer, info, sos_token=\"<sos>\"):\n",
    "    enc_translations = padded_transform(translations, input_tokenizer, info['train_target_max_pad'])\n",
    "    \n",
    "    enc_inputs = padded_transform(sentences, input_tokenizer, info['train_input_max_pad'])\n",
    "    predicted_captions = predict(enc_inputs, target_tokenizer, sos_token=sos_token)\n",
    "    padding_indices = np.argwhere(predicted_captions == target_tokenizer.texts_to_sequences([\"<eos>\"]))[:, 1]\n",
    "    \n",
    "    bleu = compute_bleu(enc_translations[:, np.newaxis, :], predicted_captions.numpy(), \n",
    "                        padding_indices=padding_indices, max_order=3)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 4.119908297317675e-24\tbleu1: 0.2565808030717997\tbleu2: 0.19657552925826305\tbleu 3: 0.16702301352822266\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate(encoder_test, target_test, input_tokenizer, target_tokenizer, info)\n",
    "print(\"bleu: {}\\tbleu1: {}\\tbleu2: {}\\tbleu 3: {}\".format(bleu[0], bleu[1][0], bleu[1][1], bleu[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, actual_translation, input_tokenizer, target_tokenizer, info):\n",
    "    enc_inputs = padded_transform([sentence], input_tokenizer, info['train_input_max_pad'])\n",
    "    predicted_arr = predict(enc_inputs, target_tokenizer)\n",
    "    predicted_sentence = target_tokenizer.sequences_to_texts(predicted_arr.numpy())\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Actual translation: %s' % (actual_translation))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <sos> Je n ' ai pas l ' intention de l ' épouser . <eos>\n",
      "Actual translation: <sos> I don ' t intend to marry him . <eos>\n",
      "Predicted translation: i don ' t know about her to be married <eos>\n",
      "\n",
      "Input: <sos> Tom ne va pas oublier cela . <eos>\n",
      "Actual translation: <sos> Tom isn ' t going to forget that . <eos>\n",
      "Predicted translation: tom won ' t do that <eos>\n",
      "\n",
      "Input: <sos> Ce curry est trop épicé . <eos>\n",
      "Actual translation: <sos> This curry is too hot . <eos>\n",
      "Predicted translation: this is too never together <eos>\n",
      "\n",
      "Input: <sos> Il n ' a pas du tout peur des serpents . <eos>\n",
      "Actual translation: <sos> He is not scared of snakes at all . <eos>\n",
      "Predicted translation: not at least he ' s at all afraid of <eos>\n",
      "\n",
      "Input: <sos> Elles s ' en fichent . <eos>\n",
      "Actual translation: <sos> They don ' t care . <eos>\n",
      "Predicted translation: they ' re away <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.randint(0, len(encoder_test), size=5)\n",
    "sample_x, sample_y = np.asarray(encoder_test)[indices], np.asarray(target_test)[indices]\n",
    "for X, y in zip(sample_x, sample_y):\n",
    "    translate(X, y, input_tokenizer, target_tokenizer, info)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes to improve\n",
    "\n",
    "- Use Beam Search.\n",
    "- Use keras.lambda on every tf operation so that we can save the model. Then, we can load the model and add attention layer output to visualize the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Further readings.\n",
    "\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "- [Tensorflow Transformer tutorial](https://www.tensorflow.org/tutorials/text/transformer)\n",
    "- [The illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Hands-on ML with Scikit-learn, keras and Tensorflow](https://github.com/ageron/handson-ml2)\n",
    "- [Python BLEU Score implementation](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
