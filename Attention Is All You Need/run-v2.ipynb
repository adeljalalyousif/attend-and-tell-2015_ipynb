{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from helpers import load_config\n",
    "from helpers.logging import print_status_bar\n",
    "from helpers.evaluation import compute_bleu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = config[\"dataset\"]\n",
    "path_to_zip = tf.keras.utils.get_file(os.path.join(os.getcwd(), \"datasets\", \"fra-eng.zip\"), origin=dataset_params[\"url\"], extract=True)\n",
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), \"fra.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # make a space between each punctionation\n",
    "    sentence = sentence.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    \n",
    "    sentence = sentence.strip()  # remove spaces\n",
    "    return sentence\n",
    "\n",
    "def preprocess_a_sentence(sentence):\n",
    "    # clean it\n",
    "    sentence = clean_sentence(sentence)\n",
    "    # add the start and end of sequences\n",
    "    return '<sos> {} <eos>'.format(sentence)\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "    # list containing a set of (input, output)\n",
    "    sentence_pairs = [[preprocess_a_sentence(sen) for sen in line.split('\\t')]  for line in lines[:num_examples]]\n",
    "    return zip(*sentence_pairs)\n",
    "\n",
    "def create_shifted_target(y):\n",
    "    \"Remove the start token and append a padding to the end.\"\n",
    "    return y[:, :-1], y[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(lang, top_k=None):\n",
    "    \n",
    "    # we are keeping the punctionation\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, filters='â€™,?!\"#$%&()*+-/:;=.@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(lang)\n",
    "    # pad the tensors\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\")\n",
    "    return sequences, tokenizer\n",
    "\n",
    "def create_dataset(X, y, batch_size=None, buffer=False, prefetch=tf.data.experimental.AUTOTUNE):\n",
    "    X_set = tf.data.Dataset.from_tensor_slices(X)\n",
    "    y_set = tf.data.Dataset.from_tensor_slices(y[0],)\n",
    "    a_set = tf.data.Dataset.zip((X_set, y_set))\n",
    "    if buffer:\n",
    "        a_set = a_set.shuffle(X[0].shape[0])\n",
    "    if batch_size is not None:\n",
    "        a_set = a_set.batch(batch_size, drop_remainder=True)\n",
    "    return a_set.repeat().prefetch(prefetch)\n",
    "\n",
    "def padded_transform(X, tokenizer, X_max):\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences(X, padding=\"post\", maxlen=X_max)\n",
    "    return X\n",
    "\n",
    "def dataset_padded_transform(X, y, X_tokenizer, y_tokenizer, X_max, y_max):\n",
    "    X = padded_transform(X, X_tokenizer, X_max)\n",
    "    y = padded_transform(y, y_tokenizer, y_max)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(input_lang, target_lang, batch_size, prefetch=tf.data.experimental.AUTOTUNE, \n",
    "            valid_size=0.15, top_k=None):    \n",
    "    \n",
    "    encoder_train, encoder_valid, target_train, target_valid = train_test_split(input_lang, \n",
    "                                                                                target_lang, \n",
    "                                                                                test_size=valid_size)\n",
    "    # build tokenizer\n",
    "    encoder_train, input_tokenizer = get_tokenizer(encoder_train, top_k=top_k)\n",
    "    target_train, target_tokenizer = get_tokenizer(target_train, top_k=top_k)\n",
    "    \n",
    "    # transform and pad\n",
    "    encoder_valid, target_valid = dataset_padded_transform(encoder_valid, target_valid,\n",
    "                                                           input_tokenizer, target_tokenizer,\n",
    "                                                           encoder_train.shape[1], target_train.shape[1])\n",
    "    \n",
    "    decoder_train, y_train = create_shifted_target(target_train)\n",
    "    train_attention_weights = np.zeros((len(decoder_train)), dtype=np.float32)\n",
    "    \n",
    "    decoder_valid, y_valid = create_shifted_target(target_valid)\n",
    "    valid_attention_weights = np.zeros((len(decoder_valid)), dtype=np.float32)\n",
    "    \n",
    "    # create dataset    \n",
    "    train_set = create_dataset((encoder_train, decoder_train),\n",
    "                               (y_train, train_attention_weights),\n",
    "                               batch_size=batch_size, buffer=True,\n",
    "                               prefetch=prefetch)\n",
    "    \n",
    "    valid_set = create_dataset((encoder_valid, decoder_valid),\n",
    "                               (y_valid, valid_attention_weights),\n",
    "                               batch_size=batch_size, prefetch=prefetch)\n",
    "    \n",
    "    # information about the training set:\n",
    "    info = dict(\n",
    "        train_size=encoder_train.shape[0],\n",
    "        train_input_max_pad=encoder_train.shape[1],\n",
    "        train_target_max_pad=target_train.shape[1],\n",
    "        valid_size=encoder_valid.shape[0],\n",
    "    )\n",
    "    return train_set, valid_set, info, input_tokenizer, target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_size': 680, 'train_input_max_pad': 13, 'train_target_max_pad': 7, 'valid_size': 120}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "batch_size = dataset_params[\"batch_size\"]\n",
    "num_examples = dataset_params[\"num_examples\"]\n",
    "top_k = dataset_params['top_common_words']\n",
    "\n",
    "# load dataset and split training, validation and testing sets.\n",
    "target_lang, input_lang = load_dataset(path_to_file, num_examples=num_examples)\n",
    "encoder_train, encoder_test, target_train, target_test = train_test_split(input_lang, \n",
    "                                                                          target_lang, \n",
    "                                                                          test_size=0.2)\n",
    "# create training and validation set\n",
    "train_set, valid_set, info, input_tokenizer, target_tokenizer = dataset(encoder_train, target_train, batch_size, \n",
    "                                                                        top_k=top_k)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_size': 680, 'train_input_max_pad': 13, 'train_target_max_pad': 7, 'valid_size': 120}\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos> je suis juste <eos>']\n",
      "[\"<sos> i ' m fair <eos>\"]\n",
      "[\"i ' m fair <eos>\"]\n"
     ]
    }
   ],
   "source": [
    "for x, y, in train_set.take(1):\n",
    "    i = 10\n",
    "    enc_x, dec_x = x\n",
    "    # y, att = y\n",
    "    print(input_tokenizer.sequences_to_texts([enc_x[i].numpy()]))\n",
    "    print(target_tokenizer.sequences_to_texts([dec_x[i].numpy()]))\n",
    "    print(target_tokenizer.sequences_to_texts([y[i].numpy()]))\n",
    "    # print(att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "N = model_config['N']\n",
    "model_depth = model_config['model_depth']\n",
    "num_heads = model_config['num_heads']\n",
    "dff = model_config['dff']\n",
    "dropout_rate = model_config['dropout_rate']\n",
    "epochs = model_config['epochs']\n",
    "\n",
    "\n",
    "steps_per_epoch = info['train_size'] // batch_size\n",
    "validation_steps = info['valid_size'] // batch_size\n",
    "max_input_vocab = len(input_tokenizer.index_word) + 1\n",
    "max_target_vocab = len(target_tokenizer.index_word) + 1\n",
    "input_max_positional_encoding = max_input_vocab\n",
    "target_max_positional_encoding = max_target_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerV2, CustomSchedule\n",
    "transformer = TransformerV2(N, model_depth, num_heads, dff, \n",
    "                            max_input_vocab, max_target_vocab, \n",
    "                            input_max_positional_encoding, target_max_positional_encoding, \n",
    "                            rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "\n",
    "Where are going to use adam optimizer with a custom LR:\n",
    "\n",
    "$$l_{\\text{rate}} = d_{\\text{model}}^{-0.5} * \\text{min}(\\text{step_num}^{-0.5}, \\text{step_num} * \\text{warmup_steps}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(model_depth)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "def neglected_loss(y_true, y_pred):\n",
    "    return tf.constant(0, dtype=tf.float32)\n",
    "\n",
    "# transformer.compile(optimizer=adam, metrics=[['accuracy'], []], loss_weights=[1., 0.],\n",
    "#                     loss=[\"sparse_categorical_crossentropy\", neglected_loss])\n",
    "transformer.compile(optimizer=adam, metrics=['accuracy'], loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 3.4205 - accuracy: 0.3971 - val_loss: 3.1324 - val_accuracy: 0.4089\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 3.3616 - accuracy: 0.3966 - val_loss: 3.0677 - val_accuracy: 0.4089\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 3.2958 - accuracy: 0.3945 - val_loss: 3.0063 - val_accuracy: 0.4089\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 3.2212 - accuracy: 0.3984 - val_loss: 2.9354 - val_accuracy: 0.4089\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 3.1708 - accuracy: 0.3969 - val_loss: 2.8618 - val_accuracy: 0.4089\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 3.0978 - accuracy: 0.4010 - val_loss: 2.7677 - val_accuracy: 0.4089\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 3.0278 - accuracy: 0.4055 - val_loss: 2.6516 - val_accuracy: 0.4089\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 2.9185 - accuracy: 0.4221 - val_loss: 2.5197 - val_accuracy: 0.4479\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 2.7907 - accuracy: 0.4794 - val_loss: 2.3827 - val_accuracy: 0.5677\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2.6791 - accuracy: 0.5380 - val_loss: 2.2651 - val_accuracy: 0.6432\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_set, steps_per_epoch=steps_per_epoch, epochs=10,\n",
    "                          validation_data=valid_set, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The following steps are used for evaluation:\n",
    "\n",
    "- Encode the input sentence using the pt tokenizer and add the necessary starting and ending tokens\n",
    "- Create a decoder input and set the first value to start token\n",
    "- Calculate the padding masks and the look ahead masks.\n",
    "- The decoder outputs the predictions by looking at the encoder's output and its own output.\n",
    "- Select the last work and calculate the argmax of that.\n",
    "- Concatentane the predicted word of input and refeed the decoder until the end token is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(enc_inputs, target_tokenizer, sos_token=\"<sos>\"):\n",
    "    y_preds = tf.fill(dims=(len(enc_inputs), 1), value=target_tokenizer.word_index[sos_token])\n",
    "    for i in range(info['train_target_max_pad']):\n",
    "        pad_size = max_target_vocab - y_preds.shape[1]\n",
    "        dec_input = tf.pad(y_preds, [[0, 0], [0, pad_size]])\n",
    "        y_probs_next = transformer.predict((enc_inputs, dec_input))\n",
    "        y_probs_next = y_probs_next[:, i:i+1]  # we only care about the current state\n",
    "        y_pred_next = tf.argmax(y_probs_next, axis=-1, output_type=tf.int32)\n",
    "        y_preds = tf.concat([y_preds, y_pred_next], axis=1)\n",
    "    return y_preds[:, 1:]  # remove the <sos> token from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 7), dtype=int32, numpy=\n",
       "array([[3, 2, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0],\n",
       "       [3, 2, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs = padded_transform(encoder_train[:10], input_tokenizer, info['train_input_max_pad'])\n",
    "predicted_captions = predict(enc_inputs, target_tokenizer)\n",
    "predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentences, translations, input_tokenizer, target_tokenizer, info, sos_token=\"<sos>\"):\n",
    "    enc_translations = padded_transform(translations, input_tokenizer, info['train_target_max_pad'])\n",
    "    \n",
    "    enc_inputs = padded_transform(sentences, input_tokenizer, info['train_input_max_pad'])\n",
    "    predicted_captions = predict(enc_inputs, target_tokenizer, sos_token=sos_token)\n",
    "    padding_indices = np.argwhere(predicted_captions == target_tokenizer.texts_to_sequences([\"<eos>\"]))[:, 1]\n",
    "    \n",
    "    bleu = compute_bleu(enc_translations[:, np.newaxis, :], predicted_captions.numpy(), \n",
    "                        padding_indices=padding_indices, max_order=3)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.0\tbleu1: 0.0\tbleu2: 0.0\tbleu 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate(encoder_train[:10], target_test[:10], input_tokenizer, target_tokenizer, info)\n",
    "print(\"bleu: {}\\tbleu1: {}\\tbleu2: {}\\tbleu 3: {}\".format(bleu[0], bleu[1][0], bleu[1][1], bleu[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, actual_translation, input_tokenizer, target_tokenizer, info):\n",
    "    predicted_arr = evaluate([sentence], [actual_translation], input_tokenizer, target_tokenizer, info)\n",
    "    predicted_sentence = target_tokenizer.sequences_to_texts(predicted_arr)\n",
    "    #predicted_sentence = \" \".join(word in predicted_sentence)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Actual translation: %s' % (actual_translation))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = encoder_test[:5], target_test[:5]\n",
    "for X, y in zip(sample_x, sample_y):\n",
    "    translate(X, y, input_tokenizer, target_tokenizer, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes to improve\n",
    "\n",
    "- Use Beam Search.\n",
    "- Use keras.lambda on every tf operation so that we can save the model. Then, we can load the model and add attention layer output to visualize the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Further readings.\n",
    "\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "- [Tensorflow Transformer tutorial](https://www.tensorflow.org/tutorials/text/transformer)\n",
    "- [The illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Hands-on ML with Scikit-learn, keras and Tensorflow](https://github.com/ageron/handson-ml2)\n",
    "- [Python BLEU Score implementation](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
